# ğŸ“Š Comparative Analysis of Loss Functions and Optimizers for CLIP Training

This research project investigates improvements to **Contrastive Languageâ€“Image Pretraining (CLIP)** by benchmarking advanced **loss functions** and **optimizers**. We propose a novel **Dynamic Temperature Scaling Loss** and evaluate its performance on multiple image-text retrieval and zero-shot classification tasks.

âœ… **Honorable Mention Winner** â€“ CLIP Training Competition (Fall 2024 Deep Learning Course)

Developed by **Ishant Kundra** and **Rahul Ravi Kadam**  
ğŸ“ Texas A&M University â€“ CSCE Deep Learning Course Project

---

## ğŸ† Recognition

> ğŸ–ï¸ **Award**: Honorable Mention at CLIP Training Competition â€“ Fallâ€™24 DL Course  
> ğŸ—“ï¸ Date: Dec 10, 2024  
> ğŸ“„ View Certificate: [Hornoral-2.pdf](./Hornoral-2.pdf)

---

## ğŸš€ Project Overview

| Item              | Description                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| ğŸ¯ **Goal**        | Improve CLIP performance through custom loss functions + optimizer tuning  |
| ğŸ§  **Losses**      | CLIP, CyCLIP, SogCLR, iSogCLR, **Dynamic Temp Loss (ours)**                 |
| âš™ï¸ **Optimizers**  | AdamW, RAdam, NovoGrad, Adafactor                                            |
| ğŸ—‚ï¸ **Datasets**    | CC3M (100K train), MSCOCO (5K eval), ImageNet (ZS)                          |
| ğŸ§ª **Evaluations** | Text-to-Image (T2I), Image-to-Text (I2T), Zero-shot classification accuracy |
| ğŸ“Š **Metrics**     | Recall@1/5/10, Top-k Accuracy, Efficiency (Time)                            |

---

## ğŸ“ Repository Structure

<pre>
ğŸ“¦ Comparative-Analysis-of-Loss-Functions-and-Optimizers-for-CLIP-Training
â”‚
â”œâ”€â”€ Best_Model-iSogCLR_new_RAdam/           # Best-performing iSogCLR+RAdam model
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ eval_isogclr_new_radam.../      # COCO & ImageNet logs
â”‚   â”‚   â””â”€â”€ isogclr_new_radam.../           # Checkpoints & config (args.json)
â”‚   â”œâ”€â”€ eval.slurm                          # HPC eval job
â”‚   â”œâ”€â”€ run.slurm                           # HPC training job
â”‚   â”œâ”€â”€ job_output_eval.slurm.12169911
â”‚   â””â”€â”€ job_output_run.slurm.12166717
â”‚
â”œâ”€â”€ Novel_Model-Dynamic_temp_scaling_loss_RAdam.zip   # ğŸ”¥ Our proposed loss implementation
â”œâ”€â”€ bimodal_exps.zip                                  # Additional bimodal training experiments
â”œâ”€â”€ Report/
â”‚   â””â”€â”€ Project_Report.pdf                            # ğŸ“„ Final detailed PDF report
â”œâ”€â”€ Honorral-2.pdf                                    # ğŸ† Award certificate
â”œâ”€â”€ Screenshots/
â”‚   â””â”€â”€ Screenshot_*.png                              # Repo preview images
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                                         # â† You're here!
</pre>

---

## ğŸ§ª Methodology

- âœ… **Model**: CLIP-style dual encoders (ViT-B/32 + DistilBERT)
- âœ… **Training Setup**: 30 epochs, batch size 128, cosine LR scheduler, warm restarts
- âœ… **Loss Exploration**:
  - `CLIP`: Standard symmetric loss
  - `CyCLIP`: Adds cyclic consistency
  - `SogCLR`: Global contrastive
  - `iSogCLR`: Adaptive temp per sample
  - `DynamicTemp`: Adapts based on variance in pos/neg pairs (new)
- âœ… **Evaluation**:
  - **MSCOCO**: T2I & I2T retrieval (Recall@1/5/10)
  - **ImageNet**: Zero-shot Top-1, Top-5
- âœ… **Metrics**: Accuracy, training time, log scaling, visualizations

---

## ğŸ“Š Key Results (from Report)

| Loss Function | Optimizer | T2I R@1 | I2T R@1 | ZS Top-1 Acc | Training Time |
|---------------|-----------|---------|---------|--------------|----------------|
| iSogCLR       | RAdam     | 14.86%  | 11.23%  | **27.03%**   | 3h 40m         |
| DynamicTemp   | RAdam     | 14.58%  | 9.32%   | 22.40%       | 3h 58m         |
| iSogCLR       | AdamW     | 14.48%  | 11.15%  | 26.74%       | 3h 23m         |
| iSogCLR       | NovoGrad  | 11.72%  | 9.98%   | 15.96%       | **3h 20m**     |

ğŸ“Œ *Detailed tables and evaluation graphs can be found in the* [Project_Report.pdf](./Report/Project_Report.pdf)

---

## ğŸ”¥ Novel Loss Function: Dynamic Temperature Scaling Loss

We propose a new contrastive loss that adaptively adjusts temperature per batch based on variance in the similarity scores between positive and negative pairs.

```python
class DynamicTemperatureScalingLoss(nn.Module):
    def __init__(self, initial_tau=0.05, alpha=0.1, min_tau=1e-3, max_tau=1.0):
        super().__init__()
        self.tau = nn.Parameter(torch.tensor(initial_tau))
        self.alpha = alpha
        self.min_tau = min_tau
        self.max_tau = max_tau

    def forward(self, image_features, text_features):
        sim_matrix = torch.matmul(image_features, text_features.T)
        pos_pairs = torch.diag(sim_matrix)
        neg_pairs = sim_matrix[~torch.eye(sim_matrix.size(0), dtype=bool)].view(sim_matrix.size(0), -1)
        pos_var = torch.var(pos_pairs, unbiased=False)
        neg_var = torch.var(neg_pairs, unbiased=False)

        with torch.no_grad():
            tau_update = self.tau * (1 + self.alpha * torch.tanh(neg_var - pos_var))
            self.tau.data = torch.clamp(tau_update, self.min_tau, self.max_tau)

        scaled_sim_matrix = sim_matrix / self.tau
        labels = torch.arange(sim_matrix.size(0)).to(image_features.device)
        return F.cross_entropy(scaled_sim_matrix, labels)

## ğŸ‘¨â€ğŸ’» Author

**Ishant Kundra**  
M.S. Computer Science, Texas A&M University  
ğŸ“¬ ishantkundra9@gmail.com
